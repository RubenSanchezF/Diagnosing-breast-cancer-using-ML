---
title: "Dynamic report k-NN"
author: "Rubén Sánchez Fernández"
lang: en # language,  en: english (default), es: español, ca: catalan, ...
date: '`r format(Sys.Date(),"%d, %B, %Y")`'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

```

```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
library(class) #k-nn function
library(gmodels) #Cross-Table function
```


```{r input, include=FALSE}
# Input / Output variables
# Tuning parameters
# ...
file1 <- "wisc_bc_data.csv"

```



#Introduction to k-NN algorithm (with code)


##Introduction

This dynamic report is created as an exercise for the Machine Learning course from Bioinformatics and Biostatistics Msc.

This report aims to introduce one of the simplest machine learning algorithms, the k-Nearest Neighbors. k-NN is a supervised, non-parametric method used both for classification and regression. It's simplicity and efectiveness makes it one of the widest used ML algorithms.

##How it works?

K-NN method is based on distance measure. Each unlabeled point is classified according to which class has the highest frequency from the _k_ nearest points. When performing regression, the output is the mean or median from the _k_ nearest points.

##Strengths and weakness table

The following table is extracted from [@lantz2015machine].


Strengths                        | Weaknesses
---------------------------------|-------------------------------------------
·Simple and effective            |·Does not produce a model, limiting the ability to understand how the features are related to the class
·Makes no assumptions about the underlying data distribution    |·Requires selection of an appropiate _k_
·Fast training phase             |·Slow classification phase
-                                |·Nominal features and missing data require additional processing
                                 

##Measuring distance

As previously mentioned, k-NN measures 'similarity' by calculating the distance between points. There are several distance measures that can be implemented with k-NN, being __Euclidean distance__ the more popular.

Eucliden distance is calculated following the next equation:

$dist(p,q) = \sqrt{ (p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_n-q_n)^2 }$

Where _p_ and _q_ are the two samples and _n_ is the feature. Therefore, $p_1$ is the point of the sample _p_ on the first feature, and $p_n$ is the point of sample _p_ on the last feature.


##Choosing the right k

When choosing what _k_ we use it is important to know that low _k_ in most cases could lead to overfitting meanwhile high _k_ could lead to underfitting. Finding the balance is key to achieve the highest accuracy possible.

In practice, usually the best approach is to choose an evaluation method and evaluate the performance of the model across different _k_ values.


##Preparing data for k-NN

We explained that k-NN is based on distance measure. Obviously, this measure is heavily dependent on what scale are the features. That's why it is important to transform the data to a standard scale across all features.

Usually, the method of rescaling for k-NN is __min-max normalization__ following the next equation:

$X_{new}= \frac{X-min(X)}{max(X)-min(x)}$

Another common method od rescaling is the __z-score standarization__:

$X_{new} = \frac{X - \mu}{\sigma} = \frac{X - mean(X)}{StdDev(X)}$



## Implementing k-NN with R

Let's create a k-NN model to perform classification.

```{r code 1}
#importing data
ds<-read.csv(file1, stringsAsFactors = FALSE)
```

```{r code 2}
print(paste0("The dataset has ", nrow(ds), " examples and ", ncol(ds), " features"))

```

Let's present an overview of the data.
```{r code 3}
#summary
summary(ds)
```

Let's also check the structure.

```{r code 4}
#internal structure
str(ds)
```

Now, we will remove the first column. Usually, the first column represents an _id_ or a sample number that doesn't provide useful information for the model. If the dataset doesn't have an _id_ feature, skip this part.

```{r code 5}
ds<-ds[-1]
```

In R, is a requirement for a lot of Machine Learning algorithms to input the target feature as a factor. 

```{r code 6}
#converting target feature to factor and changing labels
ds$diagnosis<-factor(ds$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))

```


As mentioned before, it is important to have normalized scaled data to implement k-NN. To do so, we will create a function and apply it to our dataset.

```{r code 7}
#creating a function to normalize data
normalize<-function(x){
  return(( x-min(x)) / (max(x - min(x))))
}
```

```{r code 8}
#applying the function to our data
ds_n<-as.data.frame(lapply(ds[2:31], normalize))
```

Finally, let's check that we have the data normalized.

```{r code 9}
summary(ds_n)
```

Now that we have our data ready, it is time to create the classification model. First, we will create the training and test sets. If the project already has a training and a test set, skip this part.

```{r code 10}
#training set
ds_train<-ds_n[1:469,]
#test set
ds_test<-ds_n[470:569,]

#training labels
ds_train_labels<-ds[1:469,1]
#test labels
ds_test_labels<-ds[470:569,1]
```


Next step is the training phase. K-NN is what we call a _lazy learner_, training phase only consists in storing the input data in a structured format, which is already done.

Now, we can classify the test data using the _knn()_ function from the _class_ package.

```{r code 14}
ds_test_pred<-knn(train=ds_train, test=ds_test, cl=ds_train_labels, k=21) #k=21
``` 

We obtained a vector with the predicted labels. We need to asess how true are this predictions and therefore, how accurate is the model. To do so, we can use the _CrossTable()_ function from the _gmodels_ package.

```{r code 15}
CrossTable(x = ds_test_labels, y = ds_test_pred, prop.chisq=FALSE)
```

Depending on the results we obtained, we can try to modify our model to achieve a better performance.

The easiest way to modify the model is by trying a different _k_. We must have in mind though, the possibility of overfitting and underfitting.

Another way to modify the model is by rescaling the data using _z-score standarization_ instead of standard normalization. To standarize the data we use the _scale()_ function.

```{r code 16}
ds_z<-as.data.frame(scale(ds[-1]))
```

#References

